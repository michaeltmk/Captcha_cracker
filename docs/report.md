# Captcha_cracker
A demonstration on cracking Captcha using CNN.

## Abstract
Deep learning can be used in many application and computer vision is one of the mainstream. This is a demonstration of recognizing captcha using deep neural network (DNN).
Captchas with 4 characters were scrapped on the web.
In total there are 384 captcha images, 128 for the training set, 128 for the dev set, 128 for the test set.
For each epoch, 128 more images are generated by a captcha simulator and together 128 images are fed into the model for training. The training will be stopped when the accuracy of the dev set is above 0.85.

After training the model, it was tested on the test set and had a very good result, the model is fine-tuned.

Hence the dev set is moved to the training set. The model has trained again with the same setting but a larger training set.

## Data Preparation
Captchas were scrapped on a single website. As inspected, each captcha image contains 4 characters, which can only be capital size English letters (without F, G, I, L, O, Q, Z) or numbers (3, 4, 6, 8, 9). There are 24 possibilities for each character, in other words, there are 331776 possible "answers".

In total there were 384 captcha images collected from web scrapping and labeled manually. They were separated into three sets: 128 for the training set, 128 for the dev set, 128 for the test set. This is absolutely not enough for training a  DNN, however collecting and labeling data is time costing and hence a captcha generator was used to generate new captchas for training.

The data were clean and only black and white in colour which was ready to feed into the DNN. I tried to do feature scaling using mean normalization but it did not affect the preformance(accucy of the dev set) or even worst so I dropped it, this may be because there are already batch normalization layers in the DNN model.

The training set was a combination of raw images and generated images. In each epoch, a new batch with all 128 raw images and 128 new generated images is used for training to ensure the raw images having a high enough weight in the training process. The raw/generated ratio is tuned together with the style of captcha generator as they are both causing overfitting problem of training set with respect to dev set. However, repeacting the raw images in each epoch may have high risk of overfitting these 128 images. This can verified by dev set.

### Data Preparation: Captcha Generator
In general, image data augmentation techniques such as rotation, translation, flipping, contrast adjustment or brightness adjustment are quite common in generating new images for DNN training. However, these are not useful in this case as the raw captcha images are too low in quantity and it did not collect all possible labels. Which means even applied all kind of data augmentation techniques in the raw data set, there is still bias. Besides, as inspected, the captcha images are similar in shape and style, a simulator that generates new captcha with a similar style as raw data will be more suitable.

A python library called ImageCaptcha was used. The style of the output was tuned by direct comparing with the raw images using human vision. There is no standard numarical system to comapre two images' styles and hence the tuning of generator may not be perfect. However, it will result in a clearly overfitting problem if the generated images are not similar to raw data.

## DNN Model
The DNN model had 78 CNN layers, including 5 downsampling layers which in total downscale by a factor of 32, then followed by 2 full connected layers.
Within each downsampling layer, 2 to 8 residual blocks with kernel_size 1x1,3x3,1x1 are added in order to carry the detail features along to a deeper layer.leaky_relu is used to prevent “dying ReLU” problem. 
Batch normalization is used after conv layer, as it brings significant improvements in convergence while preventing overfitting.
### Overall Architecture
![overall architecture](/images/overall_architecture.png)
### Downsampling Block
![conv block](/images/conv_block.png)
### Residual Block
![residual block](/images/residual_block.png)

## Evaluation
### Evaluation: Captcha Generator hyperparameters tuning
In order to simulate the style of raw captcha images, direct comparison and false data analysis is used to tune the hyperparameters of the generator. The default captcha building procedure was not matching with our purpose and hence some of the functions were redefined.
### Evaluation: DNN hyperparameters tuning
There are 4 hyperparameters have been tuned. They are ordered ascendingly according to the degree of importance.
* Learning rate
* Batch normalization decay
* Dropout rate
* Leaky Relu alpha
## Deployment and Result
The raw images were separated into 3 groups (training, dev, test set), each has 128 images. For each epoch, 128 new images will be generated and added to the training set. For every 10 epochs, the model will be tested on 128 new images(test1) and dev set(test2). The training will end until test2 accuracy >85%.  
![training accurancy](/images/tensorboard.png)  
As you can see the final accuracy of the training and test1 is almost 100%. Test2 is around 85%. After the model tuning, the dev set is merged into the training set and train again. The training will end until test set accuracy >85%. And Hence the final accuracy towards target captcha images is around 85%.
## Analysis and Development
Assume the base error is 100% accuracy.  
According to bias-variance decomposition, both training and test1 can easily converge to nearly >99% accuracy which means that the model is trained without any bias or variance error. Although the accuracy of test2 is high( around 85%), there is still around 10% difference between test2 and test1 accuracies. This means that there is a high variance in test1 with respect to test2. This can be caused by the difference between generated images(test1) and raw images(test2/dev set) which implies that the captcha generator should be fine-tuned.
